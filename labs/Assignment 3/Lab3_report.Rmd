---
title: "Lab3-Principle component and factor analysis"
author: "Andreas C Charitos[andch552]"
date: "2/21/2021"
header-includes:
 - \usepackage{eso-pic,graphicx,transparent}
 - \usepackage{subfig}
output: pdf_document
---


\tableofcontents

\newpage



```{r,,message=FALSE, echo=F}
# import libraries --------------------------------------------------------
library(knitr)
library(corrplot)
library(ggbiplot)
library(tidyverse)
library(gridExtra)
library(psych)
knitr::opts_chunk$set(echo = F)
```

# Problem 1 

## Data Overview

Table of the first 3 lines of the data
--------------------------------------


```{r}
# Problem 1 ---------------------------------------------------------------

# a) ---------------------------------------------------

dt = read.table("T1-9.dat")
colnames(dt) = c("Country", "100m", "200m", "400m", "800m", "1500m", "3000m", "Marathon")
kable(head(dt), caption = "First lines of national track data")

```

## a)

**Question:** Obtain the sample correlation matrix $R$ for these data and determine its eigenvalues and eigenvectors.


```{r}
# sample correlation matrix
dt_corr = cor(dt[, 2:8])
dt_eigen = eigen(dt_corr)
```

Correlation matrix
------------------

```{r}
# correlation matrix
kable(dt_corr, caption="Correlation matrix")
```

Eigenvalues
-----------

```{r}
# eigen values 
cat("Eigenvalues: \n", dt_eigen$values)
```
\vspace{125pt}

Eigenvectors
------------

```{r}
# eigen vectors
dt_eigen_vectors = dt_eigen$vectors
row.names(dt_eigen_vectors) = colnames(dt[,2:8])
colnames(dt_eigen_vectors) = c("COMP1", "COMP2", "COMP3","COMP4","COMP5","COMP6","COMP7")

kable(dt_eigen_vectors, caption = "Eigenvectors")

```


Variance importance
-------------------

```{r}
# variance importance 
pca_obj <- prcomp(dt[,2:8], scale. = F)
var_explained_dt <- data.frame(PC= paste0("PC",1:7),
                               var_explained=(pca_obj$sdev)^2/sum((pca_obj$sdev)^2))

kable(head(var_explained_dt))

```


```{r}
cat("Cumulative percentage of the total variance explained by the first two components: ", 
    sum(var_explained_dt$var_explained[1:2])*100,"%")
```

Scree plots
-----------

```{r,fig.height=6.5, fig.width=12, position="h"}
# scree plots
g1 = var_explained_dt %>%
  ggplot(aes(x=PC,y=var_explained, group=1))+
  geom_point(size=4, col='darkturquoise')+
  geom_line(col='deeppink4')+
  labs(title="Scree plot: PCA on unscaled data")

g2 = var_explained_dt %>%
  ggplot(aes(x=PC,y=var_explained))+
  geom_col(fill="cornflowerblue")+
  labs(title="Scree plot: PCA on unscaled data")+
   geom_text(aes(label = round(var_explained,4), vjust = -1))

grid.arrange(g1, g2, ncol = 2)   

```

PCA plot
--------

```{r, fig.align="center"}
## PCA computation
dt.pca = dt %>% 
  select(2:8) %>%
  prcomp(scale. = F, center = TRUE)

dt.pca %>% 
  ggbiplot::ggbiplot(scale = 1,
                     groups=dt$Country[which(unique(dt$Country)%in%c("SWE","USA"))],
                     ellipse = T)+labs(title = "PCA plot on unscaled data for Sweden and USA")

```

## b) 

**Question:** Determine the first two principal components for the standardized variables. Prepare a table showing the correlations of the standardized variables with the components and the cumulative percentage of total (standardized) sample variance explained by the two components.


First two principal components on scaled data
--------------------------------------------

```{r}
dt_scaled = scale(dt[,2:8])
dt_scaled_corr = cor(dt_scaled)
dt_scaled_eigen = eigen(dt_scaled_corr)
cat("===========================================================\n")
cat("First principal component for scaled variables: \n", dt_scaled_eigen$vectors[,1],"\n")
cat("===========================================================\n")
cat("Second  principal component for scaled variables: \n", dt_scaled_eigen$vectors[,2])
```
\vspace{65pt}

Correlation of standardized variables with components
-----------------------------------------------------

```{r}
# ------------------------------------------------
# calculate correlation of std. variables with components
e = matrix(dt_eigen$vectors[,1:2], ncol=2) 
l = diag(dt_eigen$values[1:2]%>%sqrt())
cor_mat = as.matrix(e%*%l) 
cor_dt = as.data.frame(cor_mat)
colnames(cor_dt) = c('Comp1', 'Comp2')

kable(cor_dt, caption ='Correlation of standardized variables with components')

```



Correlation matrix on scaled data
---------------------------------

```{r}
kable(dt_scaled_corr, caption="Correlation matrix scaled variables")

```


Variance importance
-------------------

```{r}
# ------------------------------------------------
pca_obj.scaled <- prcomp(dt[,2:8], scale. = T)
var_explained_dt.scaled <- data.frame(PC= paste0("PC",1:7),
                               var_explained=(pca_obj.scaled$sdev)^2/sum((pca_obj.scaled$sdev)^2))

kable(head(var_explained_dt.scaled))

```


```{r}
cat("Cumulative percentage of the total variance explained by the first two components: ", 
    sum(var_explained_dt.scaled$var_explained[1:2])*100,"%")
```
Scree plots
-----------

```{r, fig.height=6.5, fig.width=12,position="h"}
g1 = var_explained_dt.scaled %>%
  ggplot(aes(x=PC,y=var_explained, group=1))+
  geom_point(size=4, col='darkturquoise')+
  geom_line(col='deeppink4')+
  labs(title="Scree plot: PCA on scaled data")

g2 = var_explained_dt.scaled %>%
  ggplot(aes(x=PC,y=var_explained))+
  geom_col(fill="cornflowerblue")+
  labs(title="Scree plot: PCA on scaled data")+
   geom_text(aes(label = round(var_explained,4), vjust = -1))


grid.arrange(g1, g2, ncol = 2)   

```


PCA plot
--------

```{r, fig.align="center"}
## PCA computation
dt_scaled.pca = dt %>% 
  select(2:8) %>%
  prcomp(scale. = TRUE, center = TRUE)


dt_scaled.pca %>% 
  ggbiplot::ggbiplot(scale = 1,
                     groups=dt$Country[which(unique(dt$Country)%in%c("SWE","USA"))],
                     ellipse = T)+labs(title = "PCA plot on scaled data for Sweden and USA")

```

## c) 

**Question:** Interpret the two principal components obtained in Part b. (Note the first component is essentially a normalized unit vector and might measure the athletic excellence of a ginven nation. The second component might measute reative strength of a nation at the various running distances.)


**Answer:** Most of the values of the first components are pretty close. In some sense, this component measures the average time on each of the tracks. So its an equally weighted performance measure. The second component seems to be a measure of strenght regarding the distance of the runs. If the new component Y is positive, it means that nation better at shorter distances while if it’s negative, it means that it performs better at longer distances.

## d)

**Question:** Rank the nations based on their score on the first principal component. Does this ranking correspond with your intuitive notion of athletic excellence for the various countries?

Score ranking
-------------


```{r}
Y_1 = as.matrix(dt_scaled) %*% dt_scaled_eigen$vectors[, 1]
rank = list(Country=dt$Country, Score=Y_1)
rank = data.frame(rank)
ordered_idxs = order(rank$Score, decreasing=TRUE)
ordered_rank = rank[ordered_idxs, ]
# top 10 countries
#  cat("Top 10 countries: \n")
kable(ordered_rank[1:10,], caption = "Top 10 countries")

```


```{r}
# last 10 countries
# cat("Last 10 countries: \n")
kable(ordered_rank[44:54,], caption = "Last 10 countries")

```


This ranking makes sense since the countries on top are mostly developed nations who always perform well on sports while the ones at the bottom are underdeveloped nations that always lack performance on competitive sports.


```{r, fig.width=12,fig.height=8}
ordered_rank$dummy<-ifelse(ordered_rank$Score>0,"positive","negative")

ggplot(ordered_rank,aes(Country,Score,fill=dummy))+
  geom_bar(stat="identity",col="black")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+
  labs(title = "Ranking score plot")

```


# Problem 2

**Question:** Perform a factor analysis of the national track records for women data given on the previous table. Use the sample covariance matrix $S$ and interpret the factors. Compute factor scores and check for outliers in the data. Repeat the analysis with the sample correlation matrix $R$. Does it make a difference if $R$, rather than $S$, is factored?. Explain.


```{r}

# Maximum likelihood estimation and PCA for S
fit_fact_S = factanal(dt[,2:8], 
                      factors=2, 
                      covmat = cov(dt[, 2:8]))

fit_pca_S = principal(cov(dt[, 2:8]), nfactors=2, covar=T)

# Maximum likelihood estimation and PCA for R 
fit_fact_R = factanal(dt[,2:8],
                      factor=2,
                      covmat = cor(dt[,2:8]))

fit_pca_R = principal(cor(dt[,2:8]),nfactors = 2, covar = FALSE, scores = TRUE)

```



## a) Analysis for covariance matrix $S$

Maximum likelihood for covariance matrix
----------------------------------------

```{r}
fit_fact_S

# cat ("Degres of freedom: ",fit_fact_S$dof, 
#      "and fit for model is: ", fit_fact_S$criteria[1])

# scores 
fit_fact_S_scores = factanal(dt[,2:8],
                             factors=2,
                             scores="Bartlett")$scores
```
We can see that both components are explain approximately the same amount of variance. Looking at the values we see that factor1 puts more emphasis on longer distances (800m and more), while factor2 focuses mainly on shorter distances (400m and less). The model seems consistent as both components explain around 89 percent of the variance.


PCA for covariance matrix
-------------------------

```{r}
fit_pca_S

fit_pca_S$loadings

fit_pca_S_scores = factor.scores(dt[,2:8], 
                                 f=fit_pca_S)$scores
```

This time it seems that both factors focus quite a lot on the 400m track and the marathon. Component 2 puts a but more emphasis on the shorter tracks. So it seems like these components provide less interpretability. Looking at the difference we see that both components together explain less than 50 percent of the variance, which means that we lose quite a lot of the original variance. This also explains why it is quite hard to explain the components in terms of the given data.

Loadings
--------

```{r}
cov_mat_loadings = cbind(fit_fact_S$loadings,fit_pca_S$loadings) 

kable(cov_mat_loadings, caption = "Loadings for covariance matrix S from factor analysis ML and PCA")

```


Outliers plots covariance matrix
---------------------------------

```{r, fig.width=16, fig.height=12}

par(bg="whitesmoke",mfrow=c(1,2),mar=c(2,2,2,2))
# factor scores plot ML
plot(fit_fact_S_scores[,1], fit_fact_S_scores[,2],
     pch=3,
     panel.first = grid(25,25),
     main="Factor Scores (ML with Covariance)",
     xlab = "Component 1", ylab = "Component 2")
text(x = fit_fact_S_scores[,1], 
     y = fit_fact_S_scores[,2],
     labels = dt[,1], adj = c(0,1.5),cex=0.7)
# text(3, 2, "#SAM is extreme\n#KORN and COK are outiers\n#USE and KEN are on the boarder")
# We can see that the outliers are “SAM”, “KORN” and “COK”

# factors scores plot PCA
plot(fit_pca_S_scores[,1], fit_pca_S_scores[,2],
     pch=3,
     panel.first = grid(25,25),
     main="Factor Scores (PCA with Covariance)",
     xlab = "Component 1", ylab = "Component 2")
text(x = fit_pca_S_scores[,1], 
     y = fit_pca_S_scores[,2],
     labels = dt[,1], adj = c(0,1.5),cex=0.7)
# text(3, 2, "#SAM is extreme\n#KORN and COK are outiers\n#USE and KEN are on the boarder")
# The outliers this time are “MEX”, “COK” and “PNG”.
```

**According to Factors scores plot(ML with covariance)** 

* SAM is extreme 
* KORN and COK are outiers
* USE and KEN are on the boarder


**According to Factors scores plot(PCA with covariance)** 

* PNG is extreme
* MEX and COK are outliers

## b) Analysis for the correlation matrix $R$

Maximum likelihood for correlation matrix
-----------------------------------------

```{r}

fit_fact_R
#  
# cat ("Degres of freedom: ",fit_fact_R$dof, 
#      "and fit for model is: ", fit_fact_R$criteria[1])

# scores 
fit_fact_R_scores = factanal(dt[,2:8],
                             factors=2,
                             scores="Bartlett")$scores

```
Comparing with using the covariance matrix, we get the same explanation for the variables. Component 1 is still responsible for the longer tracks while component 2 is responsible for the shorter tracks. Both explain almost 90 percent of the variance which is quite good. As it seems there is no difference at all. Although, it seems there is a difference between using the covariance or correlation using the ML method.

PCA for correlation matrix
--------------------------
```{r}
fit_pca_R

fit_pca_R$loadings

fit_pca_R_scores = factor.scores(dt[,2:8], 
                                 f=fit_pca_R)$scores

```

As we see the values changed and it looks the we almost found the same components compared to the ML method in both cases (using S and R). Component 1 is again explaining the variance for the longer tracks and components to the variance for the shorter tracks. We see a slight improvement for the cumulative variance, as we actually explain more than 90 percent of the variance.

Loadings
--------

```{r}

cov_mat_loadings = cbind(fit_fact_R$loadings,fit_pca_R$loadings) 

kable(cov_mat_loadings, caption = "Loadings for correlation matrix R from factor analysis ML and PCA")

```


Outliers plots correlation matrix
---------------------------------

```{r,fig.width=16, fig.height=12}

par(bg="whitesmoke",mfrow=c(1,2))
# factor scores plot ML
plot(fit_fact_R_scores[,1], fit_fact_R_scores[,2],
     pch=3,
     panel.first = grid(25,25),
     main="Factor Scores (ML with Correlation)",
     xlab = "Component", ylab = "Component 2")
text(x = fit_fact_R_scores[,1], 
     y = fit_fact_R_scores[,2],
     labels = dt[,1], adj = c(0,1.5),cex=0.7)
# We can see that the outliers are “SAM”, “KORN” and “COK”

# factors scores plot PCA
plot(fit_pca_R_scores[,1], fit_pca_R_scores[,2],
     pch=3,
     panel.first = grid(25,25),
     main="Factor Scores (PCA with Correlation)",
     xlab = "Component 1", ylab = "Component 2")
text(x = fit_pca_R_scores[,1], 
     y = fit_pca_R_scores[,2],
     labels = dt[,1], adj = c(0,1.5),cex=0.7)
# The outliers this time are “MEX”, “COK” and “PNG”.

```


**According to Factors scores plot(ML with correlation)** 

* SAM is extreme 
* KORN and COK are outiers

**According to Factors scores plot(PCA with correlation)** 

* SAM, PNG, COK are extreme
* KORN is an outlier
* KEN seem to bee on boarder


\newpage

# Appendix 

```{r code=readLines(knitr::purl("/home/quartermaine/Courses/Multivariate-Statistical-Methods/labs/Assignment 3/Lab3_report.Rmd",documentation = 1)), echo = T, eval = F}
```
















